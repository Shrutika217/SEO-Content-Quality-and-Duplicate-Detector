{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00240cd-b466-4054-b896-f2e2c3b237dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (5.2.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: textstat in /opt/anaconda3/lib/python3.12/site-packages (0.7.10)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pyphen in /opt/anaconda3/lib/python3.12/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy beautifulsoup4 lxml nltk textstat scikit-learn sentence-transformers matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01546d09-0d40-487f-93ec-1317094a306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 rows from ../data/data.csv\n",
      "Saved parsed content -> ../data/extracted_content.csv\n",
      "Saved features -> ../data/features.csv\n",
      "Saved duplicates -> ../data/duplicates.csv (found 24 pairs at threshold 0.8)\n",
      "Adaptive thresholds -> word_count >= 6570, flesch >= 36.81\n",
      "Label distribution (adaptive):\n",
      " Low       43\n",
      "Medium    32\n",
      "High       6\n",
      "Name: quality_label, dtype: int64\n",
      "Using cv=4 for calibration due to class sizes Counter({1: 30, 2: 22, 0: 4})\n",
      "Classification report (calibrated model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.50      0.50      0.50         2\n",
      "         Low       1.00      1.00      1.00        13\n",
      "      Medium       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.80      0.80      0.80        25\n",
      "weighted avg       0.92      0.92      0.92        25\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 1  0  1]\n",
      " [ 0 13  0]\n",
      " [ 1  0  9]]\n",
      "Saved model artifacts -> ../models/quality_model.pkl\n",
      "{\n",
      "  \"url\": \"https://www.cm-alliance.com/cybersecurity-blog\",\n",
      "  \"word_count\": 1998,\n",
      "  \"readability\": 40.62,\n",
      "  \"quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://www.cm-alliance.com/cybersecurity-blog\",\n",
      "      \"similarity\": 0.998644101326733\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://cofense.com/blog\",\n",
      "      \"similarity\": 0.5708255177620432\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.theguardian.com/technology/artificialintelligenceai\",\n",
      "      \"similarity\": 0.48931339576738114\n",
      "    }\n",
      "  ],\n",
      "  \"_model_confidence_pct\": 85.51,\n",
      "  \"title\": \"Cyber Security Blog\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Full edited pipeline: improved parsing, percentile labeling, calibrated model, analyze_url()\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import textstat\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "DATA_DIR = '../data'\n",
    "RAW_CSV = os.path.join(DATA_DIR, 'data.csv')               # input (Kaggle)\n",
    "EXTRACTED_CSV = os.path.join(DATA_DIR, 'extracted_content.csv')\n",
    "FEATURES_CSV = os.path.join(DATA_DIR, 'features.csv')\n",
    "DUPLICATES_CSV = os.path.join(DATA_DIR, 'duplicates.csv')\n",
    "MODEL_DIR = '../models'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'quality_model.pkl')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load raw dataset\n",
    "df_raw = pd.read_csv(RAW_CSV)\n",
    "print(f\"Loaded {len(df_raw)} rows from {RAW_CSV}\")\n",
    "\n",
    "# 2) Improved HTML parsing: choose largest meaningful block\n",
    "def parse_html_largest_block(html):\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"\", \"\", 0\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "\n",
    "    candidates = soup.find_all(['article', 'main', 'section', 'div'])\n",
    "    best_text = \"\"\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            text = c.get_text(separator=' ', strip=True)\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "        if text and len(text) > len(best_text):\n",
    "            best_text = text\n",
    "\n",
    "    if not best_text:\n",
    "        paragraphs = [p.get_text(separator=' ', strip=True) for p in soup.find_all('p')]\n",
    "        best_text = ' '.join(paragraphs)\n",
    "\n",
    "    clean_text = ' '.join(best_text.split())\n",
    "    word_count = len(clean_text.split()) if clean_text else 0\n",
    "    return title, clean_text, word_count\n",
    "\n",
    "parsed = df_raw['html_content'].apply(lambda x: pd.Series(parse_html_largest_block(x), index=['title','body_text','word_count']))\n",
    "df = pd.concat([df_raw[['url']].reset_index(drop=True), parsed.reset_index(drop=True)], axis=1)\n",
    "df.to_csv(EXTRACTED_CSV, index=False)\n",
    "print(f\"Saved parsed content -> {EXTRACTED_CSV}\")\n",
    "\n",
    "# 3) Text cleaning & basic features\n",
    "df['clean_text'] = df['body_text'].fillna('').astype(str).str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "df['sentence_count'] = df['clean_text'].apply(lambda x: max(0, len([s for s in x.split('.') if s.strip()])))\n",
    "df['flesch_reading_ease'] = df['clean_text'].apply(lambda x: textstat.flesch_reading_ease(x) if x.strip() else 0.0)\n",
    "df['is_thin'] = df['word_count'] < 500\n",
    "\n",
    "# 4) TF-IDF vectorization and top keywords\n",
    "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_text'].fillna(''))\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_k_keywords(row_tfidf, k=5):\n",
    "    if row_tfidf.nnz == 0:\n",
    "        return ''\n",
    "    row = row_tfidf.toarray().flatten()\n",
    "    top_idx = np.argsort(row)[-k:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_idx if row[i] > 0]\n",
    "    return '|'.join(top_words)\n",
    "\n",
    "df['top_keywords'] = [get_top_k_keywords(tfidf_matrix[i], k=5) for i in range(tfidf_matrix.shape[0])]\n",
    "df_features = df[['url','word_count','sentence_count','flesch_reading_ease','is_thin','top_keywords','clean_text']].copy()\n",
    "df_features.to_csv(FEATURES_CSV, index=False)\n",
    "print(f\"Saved features -> {FEATURES_CSV}\")\n",
    "\n",
    "# 5) Duplicate detection\n",
    "sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "threshold = 0.80\n",
    "pairs = []\n",
    "n = sim_matrix.shape[0]\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        sim = float(sim_matrix[i,j])\n",
    "        if sim >= threshold:\n",
    "            pairs.append({'url1': df.loc[i,'url'], 'url2': df.loc[j,'url'], 'similarity': sim})\n",
    "dup_df = pd.DataFrame(pairs)\n",
    "dup_df.to_csv(DUPLICATES_CSV, index=False)\n",
    "print(f\"Saved duplicates -> {DUPLICATES_CSV} (found {len(dup_df)} pairs at threshold {threshold})\")\n",
    "\n",
    "# 6) Create adaptive labels\n",
    "wc_90 = max(1, int(df['word_count'].quantile(0.90)))\n",
    "flesch_60 = df['flesch_reading_ease'].quantile(0.60)\n",
    "print(f\"Adaptive thresholds -> word_count >= {wc_90}, flesch >= {flesch_60:.2f}\")\n",
    "\n",
    "def assign_quality_percentile(wc, flesch):\n",
    "    if (wc >= wc_90) and (flesch >= flesch_60):\n",
    "        return 'High'\n",
    "    elif (wc < 500) or (flesch < 30):\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "df['quality_label'] = df.apply(lambda r: assign_quality_percentile(r['word_count'], r['flesch_reading_ease']), axis=1)\n",
    "print(\"Label distribution (adaptive):\\n\", df['quality_label'].value_counts())\n",
    "\n",
    "# 7) Train calibrated RandomForest\n",
    "feature_cols = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
    "X = df[feature_cols].fillna(0).astype(float)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(df['quality_label'])\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.3, random_state=42, stratify=y_enc)\n",
    "except Exception:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.3, random_state=42)\n",
    "\n",
    "base_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# dynamically choose cv based on minimum class count\n",
    "min_class_count = min(Counter(y_train).values())\n",
    "cv_folds = min(5, min_class_count)\n",
    "print(f\"Using cv={cv_folds} for calibration due to class sizes {Counter(y_train)}\")\n",
    "\n",
    "clf = CalibratedClassifierCV(estimator=base_clf, cv=cv_folds, method='sigmoid')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_enc = clf.predict(X_test)\n",
    "y_pred_labels = le.inverse_transform(y_pred_enc)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "print(\"Classification report (calibrated model):\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump({'model': clf, 'label_encoder': le, 'vectorizer': vectorizer, 'feature_cols': feature_cols,\n",
    "             'wc_90': wc_90, 'flesch_60': flesch_60}, MODEL_PATH)\n",
    "print(f\"Saved model artifacts -> {MODEL_PATH}\")\n",
    "\n",
    "# 8) Real-time analyze_url\n",
    "def analyze_url(url, sim_threshold=0.75, top_k_similar=3, prob_threshold_high=0.65, prob_threshold_low=0.55, max_confidence=0.95):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "    except Exception as e:\n",
    "        return {'error': f'fetch_failed: {e}', 'url': url}\n",
    "\n",
    "    title, body_text, word_count = parse_html_largest_block(html)\n",
    "    clean_text = ' '.join(str(body_text).lower().split())\n",
    "    readability = textstat.flesch_reading_ease(clean_text) if clean_text else 0.0\n",
    "    sentence_count = max(0, len([s for s in clean_text.split('.') if s.strip()]))\n",
    "    is_thin = word_count < 500\n",
    "\n",
    "    feat_df = pd.DataFrame([[word_count, sentence_count, readability]], columns=feature_cols)\n",
    "\n",
    "    try:\n",
    "        pred_enc = clf.predict(feat_df)[0]\n",
    "        pred_label = le.inverse_transform([pred_enc])[0]\n",
    "        probs = clf.predict_proba(feat_df)[0]\n",
    "        pred_conf = float(probs[int(pred_enc)])\n",
    "        pred_conf = min(pred_conf, float(max_confidence))\n",
    "        quality_score_pct = round(pred_conf * 100, 2)\n",
    "    except Exception:\n",
    "        pred_label = None\n",
    "        quality_score_pct = None\n",
    "        pred_conf = 0.0\n",
    "\n",
    "    final_label = pred_label\n",
    "    if pred_label == 'High' and pred_conf >= prob_threshold_high:\n",
    "        final_label = 'High'\n",
    "    elif pred_label == 'Low' and pred_conf >= prob_threshold_low:\n",
    "        final_label = 'Low'\n",
    "    else:\n",
    "        final_label = assign_quality_percentile(word_count, readability)\n",
    "\n",
    "    similar_docs = []\n",
    "    if clean_text:\n",
    "        vec = vectorizer.transform([clean_text])\n",
    "        sims = cosine_similarity(vec, tfidf_matrix).flatten()\n",
    "        idxs = np.argsort(sims)[-top_k_similar:][::-1]\n",
    "        for idx in idxs:\n",
    "            # include the same URL\n",
    "            similar_docs.append({'url': df.loc[idx, 'url'], 'similarity': float(sims[idx])})\n",
    "\n",
    "    result = {\n",
    "        'url': url,\n",
    "        'word_count': int(word_count),\n",
    "        'readability': round(float(readability), 2),\n",
    "        'quality_label': final_label,\n",
    "        'is_thin': bool(is_thin),\n",
    "        'similar_to': similar_docs,\n",
    "        '_model_confidence_pct': quality_score_pct,\n",
    "        'title': title\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# 9) Quick test\n",
    "test_url = \"https://www.cm-alliance.com/cybersecurity-blog\"\n",
    "out = analyze_url(test_url)\n",
    "print(json.dumps(out, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045052a0-c1d0-4b6a-8bb3-0334c141c464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
